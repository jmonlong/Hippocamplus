<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      tSNE and clustering &middot; Hippocamplus
    
  </title>

  
  <link rel="stylesheet" href="../../../../css/poole.css">
  <link rel="stylesheet" href="../../../../css/syntax.css">
  <link rel="stylesheet" href="../../../../css/lanyon.css">
  <link rel="stylesheet" href="../../../../css/github.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../../../assets/hippoPlus-icon.png">
  <link rel="shortcut icon" href="../../../../assets/hippoPlus-icon.png">

  
  <link rel="alternate" type="application/rss+xml" title="RSS" href="../../../../atom.xml">
</head>

  
  <body>

    
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">


<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A place to gather useful information I keep on forgetting.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item " href="../../../../">Home</a>
    <a class="sidebar-nav-item " href="../../../../post">Posts</a>

    
    
      
        <a class="sidebar-nav-item " href="../../../../emacs/">Emacs</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../latex/">LaTeX</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../markdown/">Markdown</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../math/stats/">Math/Stats</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../python/">Python</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../r/">R</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../shell/">Shell</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../internet/">Internet</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../about/">About</a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
    
  </nav>

  <div class="sidebar-item">
    <p>&copy; 2017 Jean Monlong. All rights reserved.</p>
  </div>
</div>


    
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="../../../../" title="Home">Hippocamplus</a>
            <small>My Second Memory</small>
          </h3>
        </div>
      </div>

      <div class="container content">


<div class="post">
  <h1 class="post-title">tSNE and clustering</h1>
  <span class="post-date">Dec 2, 2017
  
  
  
  
  
  <a href='../../../../tags/r/' class='label'>R</a>
  
  <a href='../../../../tags/stats/' class='label'>stats</a>
  
  
  
  </span>
  <p>tSNE can give really nice results when we want to visualize many groups of multi-dimensional points. Once the 2D graph is done we might want to identify which points cluster in the tSNE blobs.</p>
<p>I’ll try different approaches such as hierarchical clustering, K-means, Guassian mixture and density-based clustering.</p>
<p>TL;DR Hierarchical clustering is my favorite as it’s robust, easy to use and with reasonable computing time.</p>
<pre class="r"><code>library(ggplot2)
library(dplyr)
library(magrittr)
library(ggrepel)
library(Rtsne)
library(mclust)
library(fpc)</code></pre>
<div id="data" class="section level2">
<h2>Data</h2>
<div id="normally-distributed-points" class="section level3">
<h3>Normally distributed points</h3>
<p>First, I’ll simulate an easy situation with 10 different groups. 5,000 points are distributed following Gaussian distributions in 100 dimensions. For each group, points are randomly selected as well as 3 dimensions in which they will differ from the rest.</p>
<p>Because there are 10 groups that differ in different dimensions a PCA shouldn’t be able to separate all the groups with the first two components. That’s when the tSNE comes in to do its magic (easily).</p>
<pre class="r"><code>set.seed(123456)
N = 5000
D = 100
data.norm = matrix(rnorm(N * D, 2), N)
groups.probs = runif(10)
groups = sample(1:10, N, TRUE, groups.probs/sum(groups.probs))
for (gp in unique(groups)) {
    dev = rep(1, D)
    dev[sample.int(D, 3)] = runif(3, -10, 10)
    data.norm[which(groups == gp), ] = data.norm[which(groups == 
        gp), ] %*% diag(dev)
}
info.norm = tibble(truth = factor(groups))</code></pre>
<p>The PCA and tSNE look like this:</p>
<pre class="r"><code>pca.norm = prcomp(data.norm)
info.norm %&lt;&gt;% cbind(pca.norm$x[, 1:4])
ggplot(info.norm, aes(x = PC1, y = PC2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-1.png" width="768" /></p>
<pre class="r"><code>ggplot(info.norm, aes(x = PC3, y = PC4, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-2.png" width="768" /></p>
<pre class="r"><code>tsne.norm = Rtsne(pca.norm$x, pca = FALSE)
info.norm %&lt;&gt;% mutate(tsne1 = tsne.norm$Y[, 1], tsne2 = tsne.norm$Y[, 
    2])
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-3.png" width="768" /></p>
</div>
<div id="real-data" class="section level3">
<h3>Real data</h3>
<p>As a real-life example, I use the data that motivated this exploration. It contains a bit more than 26K points and the tSNE looks like that:</p>
<pre class="r"><code>tsne.real = read.csv(&quot;https://docs.google.com/uc?id=1KArwfOd5smzuCsrpgW9Xpf9I06VOW4ga&amp;export=download&quot;)
info.real = tsne.real
ggplot(tsne.real, aes(x = tsne1, y = tsne2)) + geom_point(alpha = 0.1) + 
    theme_bw()</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/real-1.png" width="768" /></p>
</div>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2>Hierarchical clustering</h2>
<ul>
<li><strong>+</strong> Once built, it’s fast to try different number clusters.</li>
<li><strong>+</strong> Different linkage criteria to match the behavior we want.</li>
<li><strong>-</strong> Doesn’t scale well. High memory usage and computation time when &gt;30K.</li>
</ul>
<pre class="r"><code>hc.norm = hclust(dist(tsne.norm$Y))
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Complete&quot;)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-1.png" width="768" /></p>
<pre class="r"><code>hc.norm = hclust(dist(tsne.norm$Y), method = &quot;ward.D&quot;)
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Ward&quot;)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-2.png" width="768" /></p>
<p>Now on real data:</p>
<pre class="r"><code>hc.real = hclust(dist(tsne.real))
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Complete&quot;)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/hcreal-1.png" width="768" /></p>
<pre class="r"><code>hc.real = hclust(dist(tsne.real), method = &quot;ward.D&quot;)
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Ward&quot;)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/hcreal2-1.png" width="768" /></p>
<p><em>Time for this code chunk: 26.06 s</em></p>
<p>For both data, Ward gives the best clusters. For example it splits the top-left clusters better in the real data.</p>
</div>
<div id="kmeans" class="section level2">
<h2>Kmeans</h2>
<ul>
<li><strong>+</strong> Very fast.</li>
<li><strong>-</strong> Simple.</li>
</ul>
<pre class="r"><code>km.norm = kmeans(tsne.norm$Y, 9, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&quot;9 clusters&quot;)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-1.png" width="768" /></p>
<pre class="r"><code>km.norm = kmeans(tsne.norm$Y, 10, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&quot;10 clusters&quot;)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-2.png" width="768" /></p>
<p>Because it’s not working well for cluster that are not “round”, we need to ask for more clusters. In practice we’ll need to merge back together the clusters that were fragmented.</p>
<pre class="r"><code>set.seed(123456)
km.real = kmeans(tsne.real, 24, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/kmreal-1.png" width="768" /></p>
<p>Not perfect in the middle-left big cluster: cluster 11 is grabbing points from the bottom blob. Maybe increasing the number of clusters could fix this?</p>
<pre class="r"><code>set.seed(123456)
km.real = kmeans(tsne.real, 25, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/kmrealmore-1.png" width="768" /></p>
<p><em>Time for this code chunk: 9.79 s</em></p>
<p>Better. Same as with the other methods: we need to manually tweak the parameters to obtain the clustering we want…</p>
<p>Note: using several starting points help getting more robust results (<code>nstart=</code>). Increasing the number of iterations helps too (<code>iter.max=</code>).</p>
</div>
<div id="mclust" class="section level2">
<h2>Mclust</h2>
<ul>
<li><strong>+</strong> Better clusters.</li>
<li><strong>-</strong> Slow.</li>
<li><strong>-</strong> Need to be recomputed for each choice of K (number of clusters).</li>
</ul>
<pre class="r"><code>mc.norm = Mclust(tsne.norm$Y, 9)
info.norm$mclust = factor(mc.norm$classification)
mc.cent = info.norm %&gt;% group_by(mclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/mcnorm-1.png" width="768" /></p>
<p>Even the elongated cluster is nicely identified and we don’t need to split it.</p>
<pre class="r"><code>set.seed(123456)
mc.real = Mclust(tsne.real, 20, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&gt;% group_by(mclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/mcreal-1.png" width="768" /></p>
<p>Sometimes the results are a bit surprising. For example, points are assigned to cluster far away or there is another cluster in between (e.g. clusters 6 and 17). As usual changing the number of clusters helps.</p>
<pre class="r"><code>set.seed(123456)
mc.real = Mclust(tsne.real, 24, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&gt;% group_by(mclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/mcreal2-1.png" width="768" /></p>
<p><em>Time for this code chunk: 40.51 s</em></p>
<p>Note: I had to use the sub-sampling trick to speed up the process, it was taking too long. Using <code>initialization=list(subset=sample.int(nrow(tsne.real), 1000))</code>, only a 1K points are used for the EM (but all the points are assigned to a cluster at the end).</p>
</div>
<div id="density-based-clustering" class="section level2">
<h2>Density-based clustering</h2>
<ul>
<li><strong>+</strong> Can find clusters with different “shapes”.</li>
<li><strong>-</strong> More limited on real/noisy data.</li>
<li><strong>-</strong> Slow when many points.</li>
</ul>
<pre class="r"><code>ds.norm = dbscan(tsne.norm$Y, 2)
info.norm$density = factor(ds.norm$cluster)
ds.cent = info.norm %&gt;% group_by(density) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/densnorm-1.png" width="768" /></p>
<p>Woah, it found the small cluster !</p>
<pre class="r"><code>ds.real = dbscan(tsne.real, 1)
info.real$density = factor(ds.real$cluster)
ds.cent = info.real %&gt;% group_by(density) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)</code></pre>
<p><img src="../../../../post/2017-12-01-tSNEclustering_files/figure-html/densreal-1.png" width="768" /></p>
<p><em>Time for this code chunk: 104.61 s</em></p>
<p>Ouch…</p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>If possible, <strong>I’ll try to use hierarchical clustering</strong>. Especially with the Ward criterion, it worked well for both simulated and real data. Once the hierarchy is built, it’s fast to try different values for the number of clusters. Also, in the real data, I could get satisfactory results using a lower number of clusters than for the K-means (18 vs 25).</p>
<p><strong>If there are too many points</strong> (e.g. &gt;30K), hierarchical clustering might be too demanding and I would fall back to <strong>K-means</strong>. It’s fast and easy to get what we want if we accept over-fragmentation. In this cae, we might need to merged the fragments that we believe belong together but that’s not so hard to do.</p>
<p>The more advanced methods are good to keep in mind if the points ever form diverse or unusual shapes.</p>
<p>I learned two <strong>tricks to improve the performance</strong> of the methods: increasing the number of iterations and starting points for the K-means, and sub-sampling for the EM clustering.</p>
</div>

</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src="../../../../assets/highlight.min.js"></script>
    <script src="../../../../assets/r.min.js"></script>
    
    <script>
      hljs.configure({languages: ['r']});
      hljs.initHighlightingOnLoad();
    </script>
    
  </body>
</html>

