<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Regression &middot; Hippocamplus
    
  </title>

  
  <link rel="stylesheet" href="../../../../css/poole.css">
  <link rel="stylesheet" href="../../../../css/syntax.css">
  <link rel="stylesheet" href="../../../../css/lanyon.css">
  <link rel="stylesheet" href="../../../../css/github.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../../../assets/hippoPlus-icon.png">
  <link rel="shortcut icon" href="../../../../assets/hippoPlus-icon.png">

  
  <link rel="alternate" type="application/rss+xml" title="RSS" href="../../../../atom.xml">
</head>

  
  <body>

    
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">


<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A place to gather useful information I keep on forgetting.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item " href="../../../../">Home</a>
    <a class="sidebar-nav-item " href="../../../../post">Posts</a>

    
    
      
        <a class="sidebar-nav-item " href="../../../../emacs/">Emacs</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../latex/">LaTeX</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../markdown/">Markdown</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../math/stats/">Math/Stats</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../python/">Python</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../r/">R</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../shell/">Shell</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../internet/">Internet</a>
      
    
      
        <a class="sidebar-nav-item " href="../../../../about/">About</a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
    
  </nav>

  <div class="sidebar-item">
    <p>&copy; 2017 Jean Monlong. All rights reserved.</p>
  </div>
</div>


    
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="../../../../" title="Home">Hippocamplus</a>
            <small>My Second Memory</small>
          </h3>
        </div>
      </div>

      <div class="container content">


<div class="post">
  <h1 class="post-title">Regression</h1>
  <span class="post-date">Sep 16, 2017
  
  
  
  
  
  <a href='../../../../tags/stats/' class='label'>stats</a>
  
  <a href='../../../../tags/r/' class='label'>R</a>
  
  
  
  </span>
  <pre class="r"><code>library(ggplot2)
library(broom)
library(magrittr)
library(dplyr)
library(knitr)</code></pre>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<div id="one-way-or-another" class="section level3">
<h3><a href="https://youtu.be/4kg9LasvLFE">One way or another</a></h3>
<p>If we have two binary variables and we want to see if they are associated we could use a logistic regression. How do we decide which variable to be the predictor and which variable to observed variable ?</p>
<p>In theory there shouldn’t be any differences but let’s check with a dummy example:</p>
<pre class="r"><code>df = data.frame(x = sample(c(FALSE, TRUE), 100, TRUE))
df$y = df$x
df$y[1:70] = sample(c(FALSE, TRUE), 70, TRUE)

glm(y ~ x, data = df, family = binomial()) %&gt;% tidy %&gt;% 
    kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-0.3566749</td>
<td align="right">0.2845213</td>
<td align="right">-1.253597</td>
<td align="right">0.2099887</td>
</tr>
<tr class="even">
<td align="left">xTRUE</td>
<td align="right">1.1749853</td>
<td align="right">0.4207502</td>
<td align="right">2.792596</td>
<td align="right">0.0052287</td>
</tr>
</tbody>
</table>
<pre class="r"><code>glm(x ~ y, data = df, family = binomial()) %&gt;% tidy %&gt;% 
    kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-0.6931472</td>
<td align="right">0.3162277</td>
<td align="right">-2.191924</td>
<td align="right">0.0283850</td>
</tr>
<tr class="even">
<td align="left">yTRUE</td>
<td align="right">1.1749853</td>
<td align="right">0.4207502</td>
<td align="right">2.792596</td>
<td align="right">0.0052287</td>
</tr>
</tbody>
</table>
<pre class="r"><code>df$z = runif(100)
glm(y ~ x + z, data = df, family = binomial()) %&gt;% 
    tidy %&gt;% kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-0.3384519</td>
<td align="right">0.5406594</td>
<td align="right">-0.6259983</td>
<td align="right">0.5313161</td>
</tr>
<tr class="even">
<td align="left">xTRUE</td>
<td align="right">1.1705839</td>
<td align="right">0.4350359</td>
<td align="right">2.6907754</td>
<td align="right">0.0071286</td>
</tr>
<tr class="odd">
<td align="left">z</td>
<td align="right">-0.0291708</td>
<td align="right">0.7361492</td>
<td align="right">-0.0396262</td>
<td align="right">0.9683911</td>
</tr>
</tbody>
</table>
<pre class="r"><code>glm(x ~ y + z, data = df, family = binomial()) %&gt;% 
    tidy %&gt;% kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">0.3038223</td>
<td align="right">0.5173140</td>
<td align="right">0.5873074</td>
<td align="right">0.5569973</td>
</tr>
<tr class="even">
<td align="left">yTRUE</td>
<td align="right">1.1673062</td>
<td align="right">0.4343205</td>
<td align="right">2.6876608</td>
<td align="right">0.0071954</td>
</tr>
<tr class="odd">
<td align="left">z</td>
<td align="right">-1.7923109</td>
<td align="right">0.7477030</td>
<td align="right">-2.3970893</td>
<td align="right">0.0165259</td>
</tr>
</tbody>
</table>
<p>Adding another predictor doesn’t change the estimates either.</p>
</div>
<div id="interpretation" class="section level3">
<h3>Interpretation</h3>
<p>Just to make I understand the estimates correctly. It represents the log odds ratio change for each “unit” of the predictor. In the case of a binary variable, the log odds ratio between the two groups.</p>
<pre class="r"><code>glm(y ~ x, data = df, family = binomial()) %&gt;% tidy %&gt;% 
    kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-0.3566749</td>
<td align="right">0.2845213</td>
<td align="right">-1.253597</td>
<td align="right">0.2099887</td>
</tr>
<tr class="even">
<td align="left">xTRUE</td>
<td align="right">1.1749853</td>
<td align="right">0.4207502</td>
<td align="right">2.792596</td>
<td align="right">0.0052287</td>
</tr>
</tbody>
</table>
<pre class="r"><code>odds.y.ifx = mean(subset(df, x)$y)/mean(!subset(df, 
    x)$y)
odds.y.ifnotx = mean(subset(df, !x)$y)/mean(!subset(df, 
    !x)$y)
log(odds.y.ifx/odds.y.ifnotx)</code></pre>
<pre><code>## [1] 1.174985</code></pre>
</div>
<div id="extreme-cases" class="section level3">
<h3>Extreme cases</h3>
<p>How efficient is the logistic regression in cases where there is an imbalance between different types of observations ? For example if just a few genomic regions overlap an interesting annotation and I want to test is the overlap is significant.</p>
<p>Let’s look at the worst cases when there are only 1 observation for a particular class.</p>
<pre class="r"><code>df = data.frame(y = sample(c(FALSE, TRUE), 100, TRUE))
df$x = 1:nrow(df) %in% sample.int(nrow(df), 1)
glm(y ~ x, data = df, family = binomial()) %&gt;% tidy %&gt;% 
    kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">0.1010961</td>
<td align="right">0.2012644</td>
<td align="right">0.5023050</td>
<td align="right">0.6154530</td>
</tr>
<tr class="even">
<td align="left">xTRUE</td>
<td align="right">15.4649721</td>
<td align="right">1455.3975462</td>
<td align="right">0.0106259</td>
<td align="right">0.9915219</td>
</tr>
</tbody>
</table>
<p>Although the significance is low, the estimate seems quite high. I’ll repeat this process a bunch of time and with different number of supporting observations to have an idea of the distribution.</p>
<pre class="r"><code>ext.df = lapply(1:500, function(ii) {
    res = lapply(1:10, function(ssi) {
        df$x = 1:nrow(df) %in% sample.int(nrow(df), 
            ssi)
        glm(y ~ x, data = df, family = binomial()) %&gt;% 
            tidy %&gt;% mutate(rep = ii, ss = ssi)
    })
    do.call(rbind, res)
})
ext.df = do.call(rbind, ext.df)</code></pre>
<pre class="r"><code>ext.df %&gt;% filter(term == &quot;xTRUE&quot;) %&gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &quot;grey50&quot;) + facet_grid(ss ~ 
    ., scales = &quot;free&quot;) + theme_bw()</code></pre>
<p><img src="../../../../post/2017-09-16-Regression_files/figure-html/lrextsimgraph-1.png" width="672" /></p>
<p>It seems like the estimate “inflation” is problematic mostly when there are only 1 or 2 supporting observations. If there are more than 5 supporting observations the estimate is correctly centered in 0.</p>
<p>This problem is in fact called the <a href="https://en.wikipedia.org/wiki/Separation_(statistics)">problem of separation</a>. There are two approaches to deal with it:</p>
<ol style="list-style-type: decimal">
<li>Firth logistic regression.</li>
<li>Exact logistic regression.</li>
</ol>
<p>The <a href="https://cran.r-project.org/web/packages/rms/index.html"><code>rms</code> package</a> from <a href="http://www.fharrell.com/2017/01/introduction.html">Frank Harell</a>. It implements a penalized maximum likelihood estimation of the model coefficients through the <code>lrm</code> function which has a <code>penalty=</code> parameter.</p>
<pre class="r"><code>library(rms)
extrms.df = lapply(1:200, function(ii) {
    res = lapply(1:10, function(ssi) {
        res = lapply(c(1, 3, 5), function(pen) {
            df$x = 1:nrow(df) %in% sample.int(nrow(df), 
                ssi)
            cc = lrm(y ~ x, data = df, penalty = pen)$coefficient
            data.frame(term = names(cc), estimate = cc, 
                rep = ii, ss = ssi, penalty = pen, 
                stringsAsFactors = FALSE)
        })
        do.call(rbind, res)
    })
    do.call(rbind, res)
})
extrms.df = do.call(rbind, extrms.df)</code></pre>
<pre class="r"><code>extrms.df %&gt;% filter(term == &quot;x&quot;) %&gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &quot;grey50&quot;) + facet_grid(ss ~ 
    penalty, scales = &quot;free&quot;) + theme_bw()</code></pre>
<p><img src="../../../../post/2017-09-16-Regression_files/figure-html/lrmsgraph-1.png" width="672" /></p>
<p>It definitely helps: the estimates are now much closer to 0. I don’t see much difference between penalties 1, 3 or 5.</p>
<p>The <a href="https://cran.r-project.org/web/packages/logistf/index.html"><code>logistf</code> package</a>. It implements Firth’s bias reduction method with its <code>logistf</code> function.</p>
<pre class="r"><code>library(logistf)
extstf.df = lapply(1:200, function(ii) {
    res = lapply(1:10, function(ssi) {
        df$x = 1:nrow(df) %in% sample.int(nrow(df), 
            ssi)
        cc = logistf(y ~ x, data = df)$coefficient
        data.frame(term = names(cc), estimate = cc, 
            rep = ii, ss = ssi, stringsAsFactors = FALSE)
    })
    do.call(rbind, res)
})
extstf.df = do.call(rbind, extstf.df)</code></pre>
<pre class="r"><code>extstf.df %&gt;% filter(term == &quot;xTRUE&quot;) %&gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &quot;grey50&quot;) + facet_grid(ss ~ 
    ., scales = &quot;free&quot;) + theme_bw()</code></pre>
<p><img src="../../../../post/2017-09-16-Regression_files/figure-html/lrreggraph-1.png" width="672" /></p>
<p>This works well too.</p>
</div>
</div>
<div id="more-advanced-models" class="section level2">
<h2>More advanced models</h2>
<p>A dummy example with some code for Generalized Additive Models, LOESS and SVM.</p>
<pre class="r"><code>nb.samp = 1000
df = data.frame(x = runif(nb.samp, 0, 100))
df$y = rnorm(nb.samp, 0, 5) + abs(df$x - 25)
df$y = ifelse(df$x &gt; 40, rnorm(nb.samp, 0, 5) - df$x * 
    df$x/300 + 20, df$y)
ggplot(df, aes(x = x, y = y)) + geom_point(alpha = 0.5) + 
    theme_bw()</code></pre>
<p><img src="../../../../post/2017-09-16-Regression_files/figure-html/blm-1.png" width="672" /></p>
<pre class="r"><code>glm.o = glm(y ~ x, data = df)
loess.o = loess(y ~ x, data = df)
library(mgcv)
gam.o = gam(y ~ s(x, bs = &quot;cs&quot;), data = df)
library(e1071)
svm.o = svm(y ~ x, data = df)

pred.df = rbind(df %&gt;% mutate(y = predict(glm.o), model = &quot;glm&quot;), 
    df %&gt;% mutate(y = predict(gam.o), model = &quot;gam&quot;), 
    df %&gt;% mutate(y = predict(loess.o), model = &quot;LOESS&quot;), 
    df %&gt;% mutate(y = predict(svm.o), model = &quot;SVM&quot;))

ggplot(df, aes(x = x, y = y)) + geom_point(alpha = 0.2) + 
    geom_line(aes(colour = model), size = 2, alpha = 0.9, 
        data = pred.df) + theme_bw() + scale_colour_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="../../../../post/2017-09-16-Regression_files/figure-html/blmmodels-1.png" width="672" /></p>
</div>

</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src="../../../../assets/highlight.min.js"></script>
    <script src="../../../../assets/r.min.js"></script>
    
    <script>
      hljs.configure({languages: ['r']});
      hljs.initHighlightingOnLoad();
    </script>
    
  </body>
</html>

